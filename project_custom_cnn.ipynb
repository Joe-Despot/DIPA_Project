{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38357d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device used \", DEVICE)\n",
    "CLASSES = ['__background__', 'pothole']\n",
    "NUM_CLASSES = len(CLASSES)  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70850496",
   "metadata": {},
   "source": [
    "# 1 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotholeDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, limit=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = self.df['filename'].unique()\n",
    "        if limit:\n",
    "            self.image_files = self.image_files[:limit]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.image_files[idx]\n",
    "        path = os.path.join(self.image_dir, fn)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = img.resize((640, 640))\n",
    "        w, h = img.size\n",
    "        \n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        rec = self.df[self.df['filename'] == fn]\n",
    "        boxes = rec[['xmin','ymin','xmax','ymax']].values.astype(np.float32)\n",
    "\n",
    "        boxes[:, [0,2]] /= w   # x coords\n",
    "        boxes[:, [1,3]] /= h   # y coords\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones(len(rec), dtype=torch.int64)\n",
    "\n",
    "        return img_tensor, {'boxes': boxes, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d31af3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size=4, train_limit=None, valid_limit=100, test_limit=100):\n",
    "    datasets = {\n",
    "        'train': PotholeDataset(\"dataset/train/_annotations.csv\", \"dataset/train/images\", limit=train_limit),\n",
    "        'valid': PotholeDataset(\"dataset/valid/_annotations.csv\", \"dataset/valid/images\", limit=valid_limit),\n",
    "        'test':  PotholeDataset(\"dataset/test/_annotations.csv\", \"dataset/test/images\", limit=test_limit),\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        split: DataLoader(datasets[split], batch_size=batch_size, shuffle=(split == 'train' or split == 'valid' or split == 'test'),\n",
    "                          collate_fn=lambda x: tuple(zip(*x)))\n",
    "        for split in datasets\n",
    "    } \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6819bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleObjectDetector(nn.Module):\n",
    "    def __init__(self, num_preds=20):\n",
    "        super().__init__()\n",
    "        self.num_preds = num_preds\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),   # 640→320\n",
    "            nn.Conv2d(32,64,3, padding=1), nn.ReLU(), nn.MaxPool2d(2),    # 320→160\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),    # 160→80\n",
    "            nn.Conv2d(128,256,3,padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1))                                 \n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512), nn.ReLU(),\n",
    "            nn.Linear(512, num_preds * 5) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)               \n",
    "        x = self.head(x)                  \n",
    "        return x.view(-1, self.num_preds, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((640,640)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "base = \"dataset\"\n",
    "train_csv = os.path.join(base, \"train\", \"_annotations.csv\")\n",
    "val_csv   = os.path.join(base, \"valid\",   \"_annotations.csv\")\n",
    "test_csv  = os.path.join(base, \"test\",  \"_annotations.csv\")\n",
    "train_dir = os.path.join(base, \"train\", \"images\")\n",
    "val_dir   = os.path.join(base, \"valid\",   \"images\")\n",
    "test_dir  = os.path.join(base, \"test\",  \"images\")\n",
    "\n",
    "train_ds = PotholeDataset(\"dataset/train/_annotations.csv\", \"dataset/train/images\", limit=None),\n",
    "val_ds   = PotholeDataset(\"dataset/valid/_annotations.csv\", \"dataset/valid/images\", limit=None)\n",
    "test_ds  = PotholeDataset(\"dataset/test/_annotations.csv\", \"dataset/test/images\", limit=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb050005",
   "metadata": {},
   "source": [
    "# 2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b34582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_loss(preds, targets, λ_box=5.0, λ_conf=1.0):\n",
    "    B, P, _ = preds.shape\n",
    "    pred_boxes = preds[..., :4]\n",
    "    pred_conf_logits = preds[..., 4]\n",
    "\n",
    "    padded_boxes, padded_conf = [], []\n",
    "    for t in targets:\n",
    "        gt = t['boxes'].to(preds.device)\n",
    "        M = gt.size(0)\n",
    "        if M < P:\n",
    "            pad_b = torch.zeros((P-M,4), device=gt.device)\n",
    "            boxes = torch.cat([gt, pad_b], dim=0)\n",
    "            conf  = torch.cat([torch.ones(M,device=gt.device),\n",
    "                               torch.zeros(P-M,device=gt.device)])\n",
    "        else:\n",
    "            boxes = gt[:P]\n",
    "            conf  = torch.ones(P, device=gt.device)\n",
    "\n",
    "        padded_boxes.append(boxes)\n",
    "        padded_conf.append(conf)\n",
    "        \n",
    "    true_boxes = torch.stack(padded_boxes)\n",
    "    true_conf  = torch.stack(padded_conf)\n",
    "    loss_box  = F.smooth_l1_loss(pred_boxes, true_boxes)\n",
    "    loss_conf = F.binary_cross_entropy_with_logits(pred_conf_logits, true_conf)\n",
    "    return λ_box * loss_box + λ_conf * loss_conf\n",
    "\n",
    "\n",
    "def filter_predictions(pred, conf_thresh=0.3):\n",
    "    boxes  = pred[:, :4]\n",
    "    scores = torch.sigmoid(pred[:, 4])\n",
    "    mask   = scores > conf_thresh\n",
    "\n",
    "    return boxes[mask], scores[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dfe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom(model, loader, optimizer, device, epochs=10):\n",
    "    train_losses = []\n",
    "    model.to(device).train()\n",
    "    for ep in range(epochs):\n",
    "        running = 0.0\n",
    "        for imgs, targets in loader:\n",
    "            imgs = torch.stack([img.to(device) for img in imgs])\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = detection_loss(preds, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item()\n",
    "        avg = running / len(loader)\n",
    "        train_losses.append(avg)\n",
    "        print(f\"[Custom] Epoch {ep+1}/{epochs} loss: {avg:.4f}\")\n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders   = get_loaders(batch_size=4, train_limit=500)\n",
    "model     = SimpleObjectDetector(num_preds=20).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 5\n",
    "cnn_losses = train_custom(model, loaders['train'], optimizer, DEVICE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "def visualizeFasterCustomCNNredictions(\n",
    "    model, dataloader, device, num_images=10, score_threshold=0.85,\n",
    "    output_pdf_path=\"predicted_vs_gt_custom_cnn.pdf\"\n",
    "):\n",
    "    if os.path.exists(output_pdf_path):\n",
    "        os.remove(output_pdf_path)\n",
    "        print(\"File deleted.\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    images_visualized = 0\n",
    "    with PdfPages(output_pdf_path) as pdf:\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in dataloader:\n",
    "                imgs = torch.stack([img.to(DEVICE) for img in imgs])\n",
    "                targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "                preds = model(imgs)\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                for i in range(len(preds)):\n",
    "\n",
    "                    pb, ps = filter_predictions(preds[i], conf_thresh=0.3)\n",
    "                    pb[:, [0,2]] *= 640\n",
    "                    pb[:, [1,3]] *= 640\n",
    "                    img_np = imgs[i].permute(1, 2, 0).cpu().numpy()\n",
    "                    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "                    axes[0].imshow(img_np)\n",
    "                    axes[0].set_title(\"Ground Truth\")\n",
    "                    for box in targets[i]['boxes']:\n",
    "                        x1, y1, x2, y2 = [coord.item()*640 for coord in box]\n",
    "                        axes[0].add_patch(plt.Rectangle(\n",
    "                            (x1, y1), x2 - x1, y2 - y1,\n",
    "                            edgecolor='green', fill=False, linewidth=0.5\n",
    "                        ))\n",
    "                    axes[0].axis('off')\n",
    "\n",
    "                    # prediction\n",
    "                    axes[1].imshow(img_np)\n",
    "                    axes[1].set_title(\"Predictions\")\n",
    "                    for box in pb:\n",
    "                        print(\"---box:\", box)\n",
    "                        x1, y1, x2, y2 = map(float, box)\n",
    "                        axes[1].add_patch(plt.Rectangle(\n",
    "                            (x1, y1), x2 - x1, y2 - y1,\n",
    "                            edgecolor='red', fill=False, linewidth=0.5\n",
    "                        ))\n",
    "                    axes[1].axis('off')\n",
    "\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "                images_visualized += 1\n",
    "                if images_visualized >= num_images:\n",
    "                    plt.figure(figsize=(8, 5))\n",
    "                    plt.plot([ep+1 for ep in range(epochs)], cnn_losses, label='Box Loss', color='blue')\n",
    "                    plt.xlabel(\"Epoch\")\n",
    "                    plt.ylabel(\"Loss\")\n",
    "                    plt.title(\"Training Box Loss\")\n",
    "                    plt.legend()\n",
    "                    plt.grid(True)\n",
    "                    pdf.savefig()\n",
    "                    plt.close()\n",
    "\n",
    "                    plt.figure(figsize=(8, 5))\n",
    "                    plt.axis('off')\n",
    "                    plt.title(\"Evaluation Metrics\", fontsize=14)\n",
    "                    plt.text(0, 0.8, metrics_text, fontsize=12, verticalalignment='top')\n",
    "                    pdf.savefig()\n",
    "                    plt.close()\n",
    "                    print(f\"PDF report saved as {output_pdf_path}\")\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8c5c3311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File deleted.\n",
      "__________________________________\n",
      "---box: tensor([304.7130, 360.6023, 386.9023, 422.9527], device='cuda:0')\n",
      "---box: tensor([173.5348, 226.7359, 190.5000, 235.4550], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([294.8817, 347.2666, 374.5795, 407.1741], device='cuda:0')\n",
      "---box: tensor([166.9530, 217.5149, 184.4905, 226.9887], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([302.0647, 356.9124, 383.3731, 418.3499], device='cuda:0')\n",
      "---box: tensor([171.6808, 224.0974, 188.7456, 233.0272], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([307.3283, 364.1050, 390.0472, 426.9258], device='cuda:0')\n",
      "---box: tensor([175.2378, 229.1114, 191.9358, 237.6913], device='cuda:0')\n",
      "---images_visualized: 1\n",
      "__________________________________\n",
      "---box: tensor([298.2651, 351.8326, 378.7741, 412.5595], device='cuda:0')\n",
      "---box: tensor([169.2688, 220.6312, 186.4254, 229.9137], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([305.4380, 361.4156, 387.4354, 423.5823], device='cuda:0')\n",
      "---box: tensor([173.9020, 227.1535, 190.7038, 235.8205], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([306.3072, 362.7603, 388.9980, 425.6773], device='cuda:0')\n",
      "---box: tensor([174.7222, 228.2421, 191.3911, 236.9716], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([294.6887, 347.1042, 374.5100, 407.1550], device='cuda:0')\n",
      "---box: tensor([166.8618, 217.4969, 184.5533, 226.8984], device='cuda:0')\n",
      "---images_visualized: 2\n",
      "__________________________________\n",
      "---box: tensor([302.2196, 357.0996, 383.5159, 418.5437], device='cuda:0')\n",
      "---box: tensor([171.7726, 224.2095, 188.8263, 233.1135], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([304.1626, 359.7073, 385.8347, 421.4891], device='cuda:0')\n",
      "---box: tensor([173.0167, 225.9649, 189.9354, 234.7252], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([305.9983, 362.2343, 388.2277, 424.5665], device='cuda:0')\n",
      "---box: tensor([174.2748, 227.7536, 191.1108, 236.3722], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([311.9682, 370.3569, 395.7325, 434.1989], device='cuda:0')\n",
      "---box: tensor([178.2537, 233.4017, 194.7934, 241.5260], device='cuda:0')\n",
      "---images_visualized: 3\n",
      "__________________________________\n",
      "---box: tensor([300.1041, 354.1719, 380.7407, 414.9918], device='cuda:0')\n",
      "---box: tensor([170.3327, 222.1364, 187.4679, 231.2012], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([294.1974, 346.3874, 373.7741, 406.1547], device='cuda:0')\n",
      "---box: tensor([166.4152, 216.9728, 184.2645, 226.3733], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([313.5234, 372.6384, 398.2168, 437.5575], device='cuda:0')\n",
      "---box: tensor([179.6783, 235.1342, 195.7951, 243.3538], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([298.1570, 351.6998, 378.6556, 412.4158], device='cuda:0')\n",
      "---box: tensor([169.1925, 220.5513, 186.3835, 229.8248], device='cuda:0')\n",
      "---images_visualized: 4\n",
      "__________________________________\n",
      "---box: tensor([313.8007, 373.0602, 398.6372, 438.1231], device='cuda:0')\n",
      "---box: tensor([179.8345, 235.4112, 196.1255, 243.6172], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([296.6783, 349.7131, 376.8420, 410.1054], device='cuda:0')\n",
      "---box: tensor([168.1157, 219.2339, 185.7349, 228.4625], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([305.5911, 361.7678, 387.9031, 424.2077], device='cuda:0')\n",
      "---box: tensor([174.0548, 227.5191, 190.9835, 236.1686], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([311.0325, 369.0152, 394.4174, 432.4809], device='cuda:0')\n",
      "---box: tensor([177.6360, 232.3858, 194.0307, 240.6817], device='cuda:0')\n",
      "---images_visualized: 5\n",
      "__________________________________\n",
      "---box: tensor([297.8821, 351.3294, 378.3143, 411.9737], device='cuda:0')\n",
      "---box: tensor([169.0105, 220.2965, 186.2147, 229.5935], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([299.5062, 353.4980, 380.3037, 414.4592], device='cuda:0')\n",
      "---box: tensor([170.0276, 221.7963, 187.2290, 230.9730], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([314.4482, 373.6605, 398.7161, 437.9859], device='cuda:0')\n",
      "---box: tensor([179.8899, 235.6320, 196.1968, 243.6016], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([304.2448, 359.8669, 386.0981, 421.8804], device='cuda:0')\n",
      "---box: tensor([173.1839, 226.1363, 190.0062, 234.9579], device='cuda:0')\n",
      "---images_visualized: 6\n",
      "__________________________________\n",
      "---box: tensor([296.3082, 349.1891, 376.3323, 409.4339], device='cuda:0')\n",
      "---box: tensor([167.8521, 218.8527, 185.4693, 228.1171], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([314.0045, 373.0548, 398.1380, 437.2703], device='cuda:0')\n",
      "---box: tensor([179.5646, 235.2203, 195.9431, 243.1791], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([303.5359, 358.9990, 385.3778, 420.9913], device='cuda:0')\n",
      "---box: tensor([172.7055, 225.6328, 189.7684, 234.4250], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([289.6429, 340.2032, 368.0685, 398.8241], device='cuda:0')\n",
      "---box: tensor([163.3912, 212.6634, 181.4396, 222.4472], device='cuda:0')\n",
      "---images_visualized: 7\n",
      "__________________________________\n",
      "---box: tensor([296.0682, 348.8786, 376.0578, 409.0461], device='cuda:0')\n",
      "---box: tensor([167.6956, 218.6479, 185.2741, 227.9866], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([295.5858, 348.1877, 375.3903, 408.1601], device='cuda:0')\n",
      "---box: tensor([167.3681, 218.1349, 184.9122, 227.5421], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([295.8328, 348.5201, 375.7045, 408.5618], device='cuda:0')\n",
      "---box: tensor([167.5404, 218.3670, 185.0575, 227.7632], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([313.5363, 372.3425, 397.4140, 436.2970], device='cuda:0')\n",
      "---box: tensor([179.2334, 234.6541, 195.5551, 242.6891], device='cuda:0')\n",
      "---images_visualized: 8\n",
      "__________________________________\n",
      "---box: tensor([313.8234, 372.6817, 397.6704, 436.6252], device='cuda:0')\n",
      "---box: tensor([179.3837, 234.8530, 195.6908, 242.8389], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([304.1382, 359.7991, 386.1536, 422.0161], device='cuda:0')\n",
      "---box: tensor([173.1911, 226.1307, 190.0460, 235.0065], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([299.5022, 353.5695, 380.4612, 414.7283], device='cuda:0')\n",
      "---box: tensor([170.0637, 221.9276, 187.3772, 231.0603], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([317.3094, 377.8280, 403.0007, 443.7031], device='cuda:0')\n",
      "---box: tensor([182.1403, 238.6725, 198.2635, 246.6444], device='cuda:0')\n",
      "---images_visualized: 9\n",
      "__________________________________\n",
      "---box: tensor([295.2963, 347.9335, 375.3170, 408.1859], device='cuda:0')\n",
      "---box: tensor([167.2840, 218.0762, 184.9450, 227.4488], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([304.7716, 360.6847, 386.9518, 423.0081], device='cuda:0')\n",
      "---box: tensor([173.5421, 226.8090, 190.5121, 235.5273], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([300.7972, 355.2582, 381.9407, 416.6115], device='cuda:0')\n",
      "---box: tensor([170.9767, 222.9865, 187.9365, 232.1168], device='cuda:0')\n",
      "__________________________________\n",
      "---box: tensor([302.1326, 356.9539, 383.3470, 418.3108], device='cuda:0')\n",
      "---box: tensor([171.6945, 224.0835, 188.7319, 232.9981], device='cuda:0')\n",
      "---images_visualized: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "visualizeFasterCustomCNNredictions(model, loaders['valid'], DEVICE, output_pdf_path=\"predicted_vs_gt_custom_cnn.pdf\", num_images=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
