{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fd1a30",
   "metadata": {},
   "source": [
    "# DIPA Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf91501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device used \", DEVICE)\n",
    "CLASSES = ['__background__', 'pothole']\n",
    "NUM_CLASSES = len(CLASSES)  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92131cda",
   "metadata": {},
   "source": [
    "# 1 Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b63b99",
   "metadata": {},
   "source": [
    "#### Initilize bounding-box dataset class for FasterRCNN\n",
    "- _limit_ - number of images to load into dataset class\n",
    "- .csv file structure - filename, width, height, class, xmin, ymin, xmax, ymax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72dff818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class PotholeDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, limit=None, img_size=(224, 224)):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = self.df['filename'].unique()\n",
    "        if limit is not None:\n",
    "            self.image_files = self.image_files[:limit]\n",
    "\n",
    "        self.img_size = img_size  # (width, height)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_id = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, image_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Record original size and apply transform\n",
    "        orig_w, orig_h = img.size\n",
    "        img = self.transform(img)\n",
    "        new_w, new_h = self.img_size\n",
    "\n",
    "        # Load bounding boxes for this image\n",
    "        records = self.df[self.df['filename'] == image_id]\n",
    "        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values.astype(float)\n",
    "\n",
    "        # Scale boxes to match resized image\n",
    "        scale_x = new_w / orig_w\n",
    "        scale_y = new_h / orig_h\n",
    "        boxes[:, [0, 2]] *= scale_x  # xmin, xmax\n",
    "        boxes[:, [1, 3]] *= scale_y  # ymin, ymax\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Labels: 1 for pothole\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
    "            'iscrowd': torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c695607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PotholeDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, limit=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = self.df['filename'].unique()\n",
    "\n",
    "        if limit is not None:\n",
    "            self.image_files = self.image_files[:limit]  # limit number of images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, image_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        records = self.df[self.df['filename'] == image_id]\n",
    "        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': torch.tensor([idx]),\n",
    "            'area': (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),\n",
    "            'iscrowd': torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return img_tensor, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c4d2e",
   "metadata": {},
   "source": [
    "##### Retrieve the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f299bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size=16, train_limit=None, valid_limit=None, test_limit=None):\n",
    "    datasets = {\n",
    "        'train': PotholeDataset(\"dataset/train/_annotations.csv\", \"dataset/train/images\", limit=train_limit),\n",
    "        'valid': PotholeDataset(\"dataset/valid/_annotations.csv\", \"dataset/valid/images\", limit=valid_limit),\n",
    "        'test':  PotholeDataset(\"dataset/test/_annotations.csv\", \"dataset/test/images\", limit=test_limit),\n",
    "    }\n",
    "\n",
    "    loaders = {\n",
    "        split: DataLoader(datasets[split], batch_size=batch_size, shuffle=(split == 'train' or split == 'valid' or split == 'test'),\n",
    "                          collate_fn=lambda x: tuple(zip(*x)))\n",
    "        for split in datasets\n",
    "    } \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ef1ec",
   "metadata": {},
   "source": [
    "Retrieve the FasterRCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c6758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasterrcnn_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be91d2",
   "metadata": {},
   "source": [
    "# 2 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e1707",
   "metadata": {},
   "source": [
    "##### Train the FasterRCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb86f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFasterRCNN(model, dataloader, optimizer, device, epochs=2):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for imgs, targets in dataloader['train']:\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(imgs, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg = total_loss / len(dataloader)\n",
    "        train_losses.append(avg)\n",
    "        current_time = datetime.now()\n",
    "        d = current_time-start_time\n",
    "        d = str(d).split(\".\")[0]  \n",
    "        print(f\"Epoch({epoch+1}) loss: {avg}, time: {d}\") \n",
    "\n",
    "        if dataloader['test']:\n",
    "            running_val_loss = 0.0\n",
    "            for imgs, targets in dataloader['valid']:\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                print(\"targets:\", targets)\n",
    "                loss_dict = model(imgs, targets)\n",
    "                loss_val = sum(loss_val for loss_val in loss_dict.values())\n",
    "                running_val_loss += loss_val.item()\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(dataloader['valid'])\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch({epoch+1}) validation_loss: {avg_val_loss:.4f}\")\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce162da",
   "metadata": {},
   "source": [
    "### Run training for the FasterRCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joze\\Desktop\\DIPA_Project\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\joze\\Desktop\\DIPA_Project\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "loaders = get_loaders(batch_size=16, train_limit=None, valid_limit=None, test_limit=None)\n",
    "fasterrcnn = get_fasterrcnn_model(NUM_CLASSES)\n",
    "optimizer1 = torch.optim.SGD(fasterrcnn.parameters(), lr=0.005, momentum=0.9)\n",
    "epochs = 50\n",
    "fastercnn_losses, test_losses = trainFasterRCNN(fasterrcnn, loaders, optimizer1, DEVICE, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eec833",
   "metadata": {},
   "source": [
    "# 3 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738af419",
   "metadata": {},
   "source": [
    "#### Vizualize FasterRCNN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f28a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def visualizeFasterRCNNPredictions(\n",
    "    model, dataloader, device, num_images=10, score_threshold=0.70,\n",
    "    output_pdf_path=\"predicted_vs_gt_FasterRCNN.pdf\"\n",
    "):\n",
    "    if os.path.exists(output_pdf_path):\n",
    "        os.remove(output_pdf_path)\n",
    "        print(\"File deleted.\")\n",
    "\n",
    "    metric = MeanAveragePrecision(iou_thresholds=[0.5], \n",
    "                                class_metrics=True)  \n",
    "    metric.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images, targets)\n",
    "            # move everything to CPU\n",
    "            preds = [{k: v.cpu() for k, v in out.items()} for out in outputs]\n",
    "            gts   = [{k: v.cpu() for k, v in tgt.items()} for tgt in targets]\n",
    "            metric.update(preds, gts)\n",
    "\n",
    "    # 3) Compute\n",
    "    results = metric.compute() \n",
    "    images_visualized = 0\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "        \n",
    "    map = results.get('map', 0.0)\n",
    "    mar_100 = results.get('mar_100', 0.0)\n",
    "\n",
    "    metrics_text = (\n",
    "    f\"Averag recall:    {mar_100:.4f}\\n\"\n",
    "    f\"Average precision:   {map:.4f}\"\n",
    "    )\n",
    "\n",
    "    with PdfPages(output_pdf_path) as pdf:\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in dataloader:\n",
    "                imgs = list(img.to(device) for img in imgs)\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                for i in range(len(outputs)):\n",
    "\n",
    "                    img_np = imgs[i].permute(1, 2, 0).cpu().numpy()\n",
    "                    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "                    # ground truth\n",
    "                    axes[0].imshow(img_np)\n",
    "                    axes[0].set_title(\"Ground Truth\")\n",
    "                    for box in targets[i]['boxes']:\n",
    "                        x1, y1, x2, y2 = box.int().tolist()\n",
    "                        axes[0].add_patch(plt.Rectangle(\n",
    "                            (x1, y1), x2 - x1, y2 - y1,\n",
    "                            edgecolor='green', fill=False, linewidth=2\n",
    "                        ))\n",
    "                    axes[0].axis('off')\n",
    "\n",
    "                    # prediction\n",
    "                    axes[1].imshow(img_np)\n",
    "                    axes[1].set_title(\"Predictions\")\n",
    "                    for box, score in zip(outputs[i]['boxes'], outputs[i]['scores']):\n",
    "                        if score >= score_threshold:\n",
    "                            x1, y1, x2, y2 = box.int().tolist()\n",
    "                            axes[1].add_patch(plt.Rectangle(\n",
    "                                (x1, y1), x2 - x1, y2 - y1,\n",
    "                                edgecolor='red', fill=False, linewidth=2\n",
    "                            ))\n",
    "                            axes[1].text(x1, y1 - 5, f\"{score:.2f}\", color=\"red\", fontsize=8)\n",
    "                    axes[1].axis('off')\n",
    "\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "                    images_visualized += 1\n",
    "                    if images_visualized >= num_images:\n",
    "                        plt.figure(figsize=(8, 5))\n",
    "                        plt.plot([ep+1 for ep in range(epochs)], fastercnn_losses, label='train loss', color='blue')\n",
    "                        plt.plot([ep+1 for ep in range(epochs)], test_losses, label='Validation loss', color='red')\n",
    "                        plt.xlabel(\"Epoch\")\n",
    "                        plt.ylabel(\"Loss\")\n",
    "                        plt.title(\"Training Box Loss\")\n",
    "                        plt.legend()\n",
    "                        plt.grid(True)\n",
    "                        pdf.savefig()\n",
    "                        plt.close()\n",
    "\n",
    "                        plt.figure(figsize=(8, 5))\n",
    "                        plt.axis('off')\n",
    "                        plt.title(\"Evaluation Metrics\", fontsize=14)\n",
    "                        plt.text(0, 0.8, metrics_text, fontsize=12, verticalalignment='top')\n",
    "                        pdf.savefig()\n",
    "                        plt.close()\n",
    "                        print(f\"PDF report saved as {output_pdf_path}\")\n",
    "                        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File deleted.\n",
      "map: 0.2211872637271881\n",
      "map_50: 0.2211872637271881\n",
      "map_75: -1.0\n",
      "map_small: 0.24667920172214508\n",
      "map_medium: 0.17499582469463348\n",
      "map_large: 0.3931579887866974\n",
      "mar_1: 0.17164179682731628\n",
      "mar_10: 0.43283581733703613\n",
      "mar_100: 0.6156716346740723\n",
      "mar_small: 0.5581395626068115\n",
      "mar_medium: 0.6666666865348816\n",
      "mar_large: 0.7857142686843872\n",
      "map_per_class: 0.2211872637271881\n",
      "mar_100_per_class: 0.6156716346740723\n",
      "classes: 1\n",
      "PDF report saved as predicted_vs_gt_FasterRCNN.pdf\n"
     ]
    }
   ],
   "source": [
    "visualizeFasterRCNNPredictions(fasterrcnn, loaders['valid'], DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
